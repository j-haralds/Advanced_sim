{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2a: Alloy cluster expansions\n",
    "\n",
    "In this project you will explore different methods for constructing an alloy cluster expansion model for the Au-Cu alloy, using various levels of physical intuition in the model construction process.\n",
    "This physical intuition will be instilled through more and more complex priors, ranging from the completely uninformative prior of OLS to a full Bayesian analysis where the prior hyperparameters are sampled as well.\n",
    "The different cluster expansion models are then applied to the problem of predicting the ground state structure amongst a few different candidates. \n",
    "\n",
    "Note that it is not required for you to understand all the details of the cluster expansion (CE) formalism to solve the project.\n",
    "\n",
    "**You should hand in a report, via Canvas, maximum 6 pages (excluding references). \n",
    "Do not attach any appendices.\n",
    "In the report you should present your data, models and the final results.\n",
    "Do not forget to visualize your data and results.\n",
    "It is important that you discuss your findings and reflect on the results.\n",
    "In addition you need to hand in your Python code.\n",
    "Your code should run without errors and upon inspection reproduce the main results you present in the report.\n",
    "We will award extra points for code that we deem well written and structures (but we will not deduct points in the opposite case).\n",
    "Emphasis will be put on your ability to use appropriate terminology to describe and discuss the employed statistical methodologies and results.\n",
    "Please use the LaTeX template for reports that is provided on the homepage.**\n",
    "\n",
    "**<font color=red>DEADLINE: Tuesday, Dec 3, 23:59</font>**\n",
    "\n",
    "\n",
    "## Alloys\n",
    "In order to study alloys on the atomic scale one can resort to density functional theory (DFT) calculations.\n",
    "DFT can provide the energy of a given atomistic structure with high accuracy.\n",
    "The database that we provide you with is based on DFT calculations.\n",
    "\n",
    "The mixing energy per atom of a structure is defined as\n",
    "\\begin{equation}\n",
    "\\large\n",
    "    E_\\text{mix} = \\frac{E_\\text{structure} - n_\\text{Au} E_\\text{Au} - n_\\text{Cu} E_\\text{Cu}}{n_\\text{Au} + n_\\text{Cu}}\n",
    "\\end{equation}\n",
    "where $E_\\text{structure}$ is the total energy of the structure, $n_\\text{Au}$ number of Au atoms in the structure and $E_\\text{Au}$ the energy of pure Au.\n",
    "Working with the mixing energy is often convient as it is the relevant energy when considering, e.g., phase stability of alloys.\n",
    "Below a few different structures (configurations) are displayed.\n",
    "\n",
    "<img src=\"images/configurations.png\" alt=\"Drawing\" style=\"width: 750px;\"/>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## The configurational space problem\n",
    "If we consider a binary system (A-B) and a lattice consisting of 100 atoms, this yields $2^{100}\\approx 10^{30}$ unique ways of occupaying the lattice.\n",
    "While many of the structures will be duplicates when considering translation and rotational invariance we are still left with a huge number of configurations that would need to be considered for, e.g., thermodynamic averages or ground state searches.\n",
    "\n",
    "Calculating the energy of multiple structures with DFT is computationally very expensive and thus to solve this configurational space problem one often turns to building atomistic models based on DFT.\n",
    "One of the more common atomistic models to employ is the alloy cluster expansion.\n",
    "\n",
    "\n",
    "## Cluster expansions\n",
    "The alloy cluster expansion is a model based on a perfect lattice.\n",
    "This lattice can be occupied by various chemical species (Au and Cu in our case).\n",
    "In this approach the energy of the atomic structure is modeled as a sum over the orbits times their corresponding effective cluster interaction.\n",
    "In principle it looks like\n",
    "\\begin{equation}\n",
    "\\large\n",
    "E_{mix} = J_0 + \\sum_\\alpha m_\\alpha J_\\alpha,\n",
    "\\end{equation}\n",
    "where $\\alpha$ refers to different orbits such as nearest neighbor pairs, next nearest neighbor pairs, triplets etc, $m_\\alpha$ is the multiplicity, i.e. the number of symmetry equivalent clusters in the orbit per unit cell, and $J_\\alpha$ is the corresponding effective cluster interaction (ECI).\n",
    "In this project, we are not concerned with the details of decomposing a structure into orbits but simply use the [`icet` package](https://icet.materialsmodeling.org) for this purpose.\n",
    "\n",
    "<img src=\"images/clusters_v1.png\" alt=\"Drawing\" style=\"width: 650px;\"/>\n",
    "\n",
    "\n",
    "Instead we focus on how to determine the unknown parameters (ECIs) for the cluster expansion model, and how to choose which type of clusters (orbits) to include in the model.\n",
    "The equation above can be formulated as a dot product by defining $\\boldsymbol{\\xi}=[1, N_1, N_2, ...]$ and $\\boldsymbol{J}=[J_0, J_1, J_2, ...]$ as $E_{mix} = \\boldsymbol{\\xi} \\boldsymbol{J}$.\n",
    "Here, $\\boldsymbol{\\xi}$ is often refered to the cluster vector of the given configuration.\n",
    "\n",
    "If the mixing energy, $E_{mix}$, is provided, e.g., from DFT calculations, for a set of configurations, then the problem of finding the ECIs, $\\boldsymbol{J}$, can be cast in the form of OLS\n",
    "\\begin{equation}\n",
    "\\large\n",
    "\\boldsymbol{J}_{opt} = \\min \\boldsymbol{J} || \\boldsymbol{X} \\boldsymbol{J} - \\boldsymbol{E_{mix}} ||_2 ^2,\n",
    "\\end{equation}\n",
    "where $\\boldsymbol{E}_{mix}$ is a vector containing the mixing energies for all structures and $\\boldsymbol{X}$ a matrix where each row corresponds to a cluster vector one of the given structures.\n",
    "\n",
    "\n",
    "## The linear problem\n",
    "Simplest way to solve the linear problem would be to use ordinary least-squares (OLS). However, OLS tends to overfit and thus linear regression with regularization such as Ridge and LASSO are feasible alternatives.\n",
    "\n",
    "A common issue in finding the ECIs is to select an approriate number of features (clusters) to include in the model. Features in our problem would corresponds to clusters with various interatomic distances and order. Too few can lead to underfitting and too many to overfitting. \n",
    "\n",
    "In this project you will construct cluster-expansion model using linear regression methods such as Lasso and ARDR. You will use cross-validation but also information criteria such as AIC and BIC in order to evaluate how many features is reasonable to include in your cluster expansion. For more information and details about these methods and metrics, see the lecture notes.\n",
    "\n",
    "Lastly, you will see how bayesian parameter estimation of the ECIs can be used for cluster-expansion.\n",
    "\n",
    "### Cross validation\n",
    "You may freely chose a suitable cross-valdation method to use for the following tasks. K-fold is commonly used and available through `sklearn` (https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html). Ten folds might be a good starting point to try out.\n",
    "\n",
    "### Information criteria\n",
    "The Bayesian information criteria (BIC) is defined as\n",
    "\\begin{equation}\n",
    "\\text{BIC} = 2 log(L_*) - N_p log(N_d)\n",
    "\\end{equation}\n",
    "where $L_*$ is the maximized likelihood, $N_p$ the number of parameters, and $N_d$ number of data points.     \n",
    "The energies (or data) can be modeled through\n",
    "Here we assume the errors, $\\epsilon$, are normal distributed with zero mean and variance $\\sigma^2$.\n",
    "\\begin{equation}\n",
    "\\boldsymbol{E} = \\boldsymbol{\\xi} \\boldsymbol{J} + \\epsilon , \\quad \\epsilon \\sim \\mathcal{N}(\\mu=0, \\sigma^2) \\\\\n",
    "\\end{equation}\n",
    "Therefore the likelihood, $P(D|\\boldsymbol{J}, \\sigma)$, can be defined as\n",
    "\\begin{equation}\n",
    "L = P(D|\\boldsymbol{J}, \\sigma) = \\frac{1}{(2\\pi \\sigma^2)^{N_d/2}}\\exp{(-||\\boldsymbol{X} \\boldsymbol{J}-\\boldsymbol{E} ||^2 / 2\\sigma^2)}\n",
    "\\end{equation}\n",
    "The ECIs, $\\boldsymbol{J}$, you will obtain from linear regression algorithms (LASSO, etc), and $\\sigma$ can be found from setting $\\frac{\\mathrm{d} L }{\\mathrm{d} \\sigma} = 0$. This is left as an exercise to the reader but final expression for BIC becomes\n",
    "\\begin{equation}\n",
    "\\text{BIC} = -N_d log(\\text{MSE}) - N_p log(N_d) + \\text{const}\n",
    "\\end{equation}\n",
    "where the constant term, $\\text{const}$, is irrelevant and $\\text{MSE}$ is the mean squared error, defined as \n",
    "\\begin{equation}\n",
    "\\text{MSE} = \\frac{1}{N_d}||\\boldsymbol{X} \\boldsymbol{J}-\\boldsymbol{E} ||^2\n",
    "\\end{equation}\n",
    "\n",
    "### sklearn\n",
    "Regression algorithms like OLS, Ridge, ARDR (and many more) are available in `sklearn`. One can also use the [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) from `sklearn` to standardize data. \n",
    "\n",
    "*Note: Since we're including the constant term in our cluster vectors we need to set `fit_intercept=False` when using the linear models from sklearn.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional information\n",
    "For more information, see the lecture notes as well as the demos. \n",
    "For Task 3, a similar analysis is performed in the notebook `ce-with-covariance-matrix.ipynb`.\n",
    "\n",
    "Additional information in regards to cluster expansions and how these can be trained and used can be found in the  following papers:\n",
    "\n",
    "* Robust data-driven approach for predicting the configurational energy of high entropy alloys (doi.org/10.1016/j.matdes.2019.108247)\n",
    "* [`icet`](https://icet.materialsmodeling.org) – A Python Library for Constructing and Sampling Alloy Cluster Expansions (doi.org/10.1002/adts.201900015)\n",
    "* Covariance regularization by Mueller et al. [Physical Review B **80**, 024103 (2009)](https://journals.aps.org/prb/abstract/10.1103/PhysRevB.80.024103)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, you are provided with data in the form of [ASE databases](https://wiki.fysik.dtu.dk/ase/ase/db/db.html). Please see the notebook `introduction-to-ase.ipynb` for further details on how to read such databases and an introduction to ASE in general, and the notebook `introduction-to-cluster-expansions.ipynb` for tips on working with cluster expansions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ase\n",
    "import icet\n",
    "from ase.db import connect\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy \n",
    "import sklearn\n",
    "sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Prepare data (1.5p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data can be found in the ASE database `AuCu-structures.db`.\n",
    "Here each row corresponds to an atomic structure with an attached mixing energy in units of eV per atom.\n",
    "Use fixed cutoff radii of 8 Å, 6 Å, and 5 Å for pairs, triplets, and quadruplets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** \n",
    "* Read the database and collect the atomic structures and energies.\n",
    "* Plot the mixing energy as a function of Cu concentration\n",
    "* Standardize the mixing energy $E$ and the cluster vectors $X$. \n",
    "\n",
    "**Discuss:**\n",
    "* Why is it good practice to standardize the data? Is it necessary in this case?\n",
    "\n",
    "*Hints:*\n",
    "- The mixing energy for a structure can be retrieved by accessing the `mixing_energy` field on the row corresponding to that structure, like `E_mix = row.mixing_energy`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ase db AuCu-structures.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 52 structures.\n",
      "Mixing energy mean: -0.0314 eV/atom\n"
     ]
    }
   ],
   "source": [
    "# Read ASE database and collect structures + mixing energies\n",
    "db_path = 'AuCu-structures.db'\n",
    "db = connect(db_path)\n",
    "\n",
    "structures = []\n",
    "mixing_energies = []\n",
    "metadata = []\n",
    "\n",
    "for row in db.select():\n",
    "    atoms = row.toatoms()\n",
    "    structures.append(atoms)\n",
    "    E_mix = row.mixing_energy\n",
    "    mixing_energies.append(E_mix)\n",
    "    metadata.append({\n",
    "        'row_id': row.id,\n",
    "        'formula': row.formula,\n",
    "        'natoms': row.natoms,\n",
    "        'mass': row.mass\n",
    "    })\n",
    "\n",
    "mixing_energies = np.asarray(mixing_energies)\n",
    "\n",
    "print(f'Loaded {len(structures)} structures.')\n",
    "print(f'Mixing energy mean: {mixing_energies.mean():.4f} eV/atom')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[ 0.0030715  -0.00267067  0.01279463 -0.01518744 -0.02093657 -0.05223576\n -0.08240353 -0.01065374 -0.04427555 -0.00382364 -0.01904572 -0.0094087\n -0.05243008 -0.07793217 -0.07991778 -0.00093488 -0.05410745 -0.04850531\n -0.01503727  0.0063445  -0.03696611 -0.05443886 -0.04523728  0.00360363\n -0.04783518 -0.00396916 -0.00707355 -0.00560239 -0.00562975 -0.00206933\n -0.00968542 -0.0374908  -0.05895944 -0.06631033 -0.05651413 -0.05666618\n -0.00917238 -0.03256556 -0.02713585 -0.05401702 -0.04506241 -0.04407154\n -0.00987322  0.004874   -0.05521559  0.00616353 -0.07217434 -0.02855596\n -0.0262728  -0.04604518 -0.06787612 -0.068738  ].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m scaler \u001b[38;5;241m=\u001b[39m sklearn\u001b[38;5;241m.\u001b[39mpreprocessing\u001b[38;5;241m.\u001b[39mStandardScaler()\n\u001b[0;32m----> 2\u001b[0m E_scaled \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmixing_energies\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.13/site-packages/sklearn/preprocessing/_data.py:894\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 894\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.13/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.13/site-packages/sklearn/preprocessing/_data.py:930\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[1;32m    899\u001b[0m \n\u001b[1;32m    900\u001b[0m \u001b[38;5;124;03mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;124;03m    Fitted scaler.\u001b[39;00m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    929\u001b[0m first_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 930\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    938\u001b[0m n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    940\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/myenv/lib/python3.13/site-packages/sklearn/utils/validation.py:2944\u001b[0m, in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2942\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m   2943\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m-> 2944\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2945\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[1;32m   2946\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m~/myenv/lib/python3.13/site-packages/sklearn/utils/validation.py:1093\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1086\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1087\u001b[0m             msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1088\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124marray=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1089\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1090\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1091\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif it contains a single sample.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1092\u001b[0m             )\n\u001b[0;32m-> 1093\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype_numeric \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(array\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkind\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSV\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1096\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1097\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1098\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1099\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[ 0.0030715  -0.00267067  0.01279463 -0.01518744 -0.02093657 -0.05223576\n -0.08240353 -0.01065374 -0.04427555 -0.00382364 -0.01904572 -0.0094087\n -0.05243008 -0.07793217 -0.07991778 -0.00093488 -0.05410745 -0.04850531\n -0.01503727  0.0063445  -0.03696611 -0.05443886 -0.04523728  0.00360363\n -0.04783518 -0.00396916 -0.00707355 -0.00560239 -0.00562975 -0.00206933\n -0.00968542 -0.0374908  -0.05895944 -0.06631033 -0.05651413 -0.05666618\n -0.00917238 -0.03256556 -0.02713585 -0.05401702 -0.04506241 -0.04407154\n -0.00987322  0.004874   -0.05521559  0.00616353 -0.07217434 -0.02855596\n -0.0262728  -0.04604518 -0.06787612 -0.068738  ].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "E_scaled = scaler.fit(mixing_energies)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: OLS and ridge regression (2p)\n",
    "\n",
    "Use OLS and ridge to fit the ECIs. You may use routines from, e.g., scikit-learn.\n",
    "\n",
    "Ridge regression uses a hyperparameter $\\alpha$ to penalize large values for the ECIs, by using the following loss function: \n",
    "\n",
    "$$ \n",
    "L = ||E - XJ||^2 + \\alpha ||J||^2\n",
    "$$\n",
    "\n",
    "where $E$ is the mixing energy and $x$ the design matrix of cluster vectors and $J$ the ECIs. The solution $J_{opt}$ that minimizes $L$  can then be written as:\n",
    "\n",
    "$$ \n",
    "J_{opt, Ridge} = (X^T X + \\alpha I)^{-1} X^T E\n",
    "$$\n",
    "\n",
    "**Tasks**\n",
    "* Fit ECIs using OLS and ridge regression\n",
    "    * Use k-fold cross validation to find a good value for $\\alpha$. \n",
    "* Compare the CV-RMSE and size of the ECIs between OLS and Ridge. The ECIs can for instance be plotted in a bar chart.\n",
    "\n",
    "**Discuss**\n",
    "* What are the qualitative differences between OLS and Ridge?\n",
    "\n",
    "*Note: Since the constant term is included in our cluster vectors we need to set `fit_intercept=False` when using the linear models from sklearn.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Covariance matrix for cluster similarity (4p)\n",
    "\n",
    "Ridge regression regularizes parameters by using the $\\alpha$ to penalize larger absolute values of the ECIs. This is a very general approach that works for a broad range of problems, and can be motivated by physical intuition in that smaller parameter values are favored. However, we can improve upon Ridge regression by creating more elaborate regularization schemes that inlcude physical intuition about our specific system. \n",
    "\n",
    "One example of such physical intuition is that we expect cluster orbits with more sites and with a larger distance between them to contribute less to the overall energy of the structure than smaller/more compact clusters. In other words, we expect that the ECIs decrease with the number of sites and the size of a orbit. Following the approach outlined Mueller et al. in [Physical Review B **80**, 024103 (2009)](https://journals.aps.org/prb/abstract/10.1103/PhysRevB.80.024103), this can be achieved by modifying the Ridge least-squares estimator as follows: \n",
    "\n",
    "$$\n",
    "J_{opt, Cov} = (X^T X + \\Lambda)^{-1} X^T E.\n",
    "$$\n",
    "\n",
    "Here, $\\Lambda$ is known as the *regularization matrix*. Using this regularization matrix is the same as using the following prior over the ECIs\n",
    "\n",
    "$$\n",
    "P(J|X) \\propto e^{-J^T \\Lambda J /2}.\n",
    "$$\n",
    "\n",
    "This is a multivariate normal distribution over the components of $J$ with covariance $\\Lambda^{-1}$, and hence we can interpret $\\Lambda$ to be the inverse of the covariance matrix for the prior distribution  of $J$. By modifying $\\Lambda$, we can thus encode physical intuition similarly to how we usually specify priors in a fully Bayesian approach. \n",
    "\n",
    "The elements of $\\Lambda$ are given by\n",
    "$$\n",
    "\\Lambda_{\\alpha,\\alpha} = \\frac{\\sigma^2}{\\sigma_\\alpha^2} + \\sum_{\\beta | \\beta\\neq\\alpha} \\frac{\\sigma^2}{\\sigma_{\\alpha\\beta}}\\\\\n",
    "\\Lambda_{\\alpha\\beta} = \\Lambda_{\\beta\\alpha} = - \\frac{\\sigma^2}{\\sigma_{\\alpha\\beta}^2}.\n",
    "$$\n",
    "\n",
    "It is useful to consider the extremal cases for the elements of $\\Lambda$, using the reduced values $\\lambda_{\\alpha}=\\sigma^2/\\sigma_\\alpha^2$ and  $\\lambda_{\\alpha\\beta}=\\sigma^2/\\sigma_{\\alpha\\beta}^2$.\n",
    "\n",
    "1. $\\lambda_{\\alpha} \\rightarrow \\infty$: force ECI for orbit $\\alpha$ to zero (= remove orbit)\n",
    "1. $\\lambda_{\\alpha} \\rightarrow 0$ and $\\lambda_{\\alpha\\beta} \\rightarrow 0$: all ECIs/orbits are equally likely; this recovers OLS\n",
    "1. $\\lambda_{\\alpha\\beta} \\rightarrow 0$: no correlation (coupling) between orbits; this recovers ridge regression if $\\lambda_\\alpha$ is the same for all orbits\n",
    "1. $\\lambda_{\\alpha\\beta} \\rightarrow \\infty$: force two orbits to have the same ECI\n",
    "\n",
    "In this project we will only focus on the diagonal elements $\\lambda_\\alpha$, and set the off-diagonal elements $\\lambda_{\\alpha\\beta}=0$. We will thus not merge any cluster orbits. Merging cluster orbits has the benefit of constraining the parameter space, simplifying optimization procedure. See the demo `cluster-expansion-construction` for details.\n",
    "\n",
    "We can thus encode our prior information/physical intuition that orbits with more sites/larger size should have smaller ECIs by modifying the parameters $\\lambda_\\alpha$. To demonstrate this approach, we calculate $\\lambda_\\alpha$ according to the following (linear) regularization scheme\n",
    "\n",
    "$$ \n",
    "\\lambda_\\alpha(n,r, \\gamma) = \\gamma_1r+\\gamma_2n, \n",
    "$$\n",
    "\n",
    "where $n$ and $r$ is the number of sites and radius of orbit $\\alpha$ respectively. $\\lambda_\\alpha$ grows with larger $n$ and $r$, decreasing the size of that ECI. The problem is now reduced to finding the two optimal hyperparameters $\\gamma$, instead of finding all the $\\lambda_\\alpha$ individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks:**\n",
    "* Find the hyperparameters $\\gamma$ that minimizes the CV-RMSE of the model with $J_{opt, Cov}$.\n",
    "* Compare the CV-RMSE and the ECIs to OLS and Ridge from Task 2. \n",
    "\n",
    "**Discuss:**\n",
    "* What is the interpretation of having an individual parameter $\\lambda_\\alpha$ for each orbit? How does this compare to Ridge regression?\n",
    "* How does the CV-RMSE and the ECIs compare to OLS and Ridge? Can you explain it? \n",
    "\n",
    "*Hints*\n",
    "* You can use, e.g., [`scipy.minimize.optimize`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html) to optimize the hyperparameters $\\gamma$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Bayesian Cluster expansion (4.5p)\n",
    "\n",
    "\n",
    "A full Bayesian analysis includes the hyperparameters for the prior distribution as parameters to be optimized, which means that they are also sampled and optimized in conjunction with the model parameters. This can be compared to the Covariance approach in Task 3, which finds a single optimal prior distribution. \n",
    "\n",
    "In this task, you will perform a full Bayesian analysis for the ECIs.\n",
    "\n",
    "### Priors\n",
    "For this problem you can use a gaussian prior for the ECIs with a zero mean and variance $\\alpha ^2$. This can be defined as\n",
    "\\begin{equation}\n",
    "P(\\boldsymbol{J}) = \\frac{1}{(2\\pi \\alpha^2)^{N_p/2}}\\exp{(-||\\boldsymbol{J}||^2 / 2\\alpha^2)}\n",
    "\\end{equation}\n",
    "where $N_p$ is the number of ECIs (length of vector $\\boldsymbol{J}$). \n",
    "\n",
    "We assume homoscedastic errors in this task, and let $\\sigma$ be the standard deviation in the likelihood.\n",
    "For the priors over $\\sigma$ and $\\alpha$ you can for example use inverse gamma distributions. The total prior $P(\\boldsymbol{J}, \\sigma, \\alpha)$  is then given by the product of all priors.\n",
    "\n",
    "### Posterior\n",
    "In this task you will generate cluster expansion models from the posterior probability distribution $P(\\boldsymbol{J}|D)$, where $D$ is the training data (cluster-vectors and energies). The posterior is defined as\n",
    "\\begin{equation}\n",
    "\\underbrace{P(\\boldsymbol{J}, \\sigma^2, \\alpha|D)}_\\text{posterior} = \\underbrace{P(D|\\boldsymbol{J}, \\sigma)}_\\text{likelihood} \\underbrace{P(\\boldsymbol{J}, \\sigma, \\alpha)}_\\text{prior}/P(D)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "**Tasks:**\n",
    "* Define the (log) liklihood, (log) prior and (log) posterior functions.\n",
    "* Sample the posterior distribution using MCMC and generate about 100-1000 (uncorrelated) samples (models).\n",
    "* Visualize the ECIs for a MCMC chain.\n",
    "\n",
    "**Discuss:**\n",
    "* How many parameters seems \"necessary\" (non-zero) according to your MCMC sampling?\n",
    "* What would happen if you had set your priors to something \"unphysical\", for instance to favor 3rd and 4th order clusters very highly?\n",
    "\n",
    "*Hints:*\n",
    "* `emcee` can be used for the MCMC sampling\n",
    "* You may need to use a long burn-in period\n",
    "* From previous tasks we know and $\\sigma$ to be in the range 0.02-2 and $\\alpha$ to be in the range 0.05-0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you are using `emcee` for the MCMC sampling, please use the following code snippet when defining the sampler.**\n",
    "This uses the `HDF` backend, which saves the sampling chain to a `.h5`-file instead of keeping it in memory. \n",
    "This is especially important if you are using the JupyterHub server, as otherwise the memory may fill up, decreasing the performance for everyone using the server. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emcee\n",
    "sampler = emcee.EnsembleSampler(backend=emcee.backends.HDFBackend('chain_task3.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: ARDR Feature selection (4p)\n",
    "\n",
    "The techniques previously studied (Task 2-4) have been rather hands-on in that the user has quite a lot of control over how the parameter priors are constructed and optimized. Automatic Relevance Detection Regression (ARDR) on the other hand offers an automatic approach to optimizing the shape of the parameter priors, through several different shape parameters. In this task you will investigate how ARDR performs while varying a single shape parameter known as `threshold_lambda` in the [scikit-documentation for ARDR](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ARDRegression.html).  \n",
    "\n",
    "\n",
    "Scan a range of values for the hyper-parameter and evaluate CV-error and IC to determine a good value for the hyper-parameters. Records also how many nonzero parameters you obtained.\n",
    "\n",
    "To demonstrate the power of ARDR, increase the cutoffs in the cluster space to `[13,8,6]`, which greatly increases the number of parameters and makes the problem very overdetermined. \n",
    "\n",
    "**Tasks:**\n",
    "* Compute the training and CV error for a range of values for `threshold_lambda`.\n",
    "* Visualize the training error and CV-error as well as the AIC/BIC as function of number of non-zero parameters in the model.   \n",
    "\n",
    "**Discuss:**\n",
    "* How many features you think is suitable to include in a final model based on your analysis.\n",
    "* Which ECIs are selected here? Is there a difference to OLS/Ridge/Covariance approach?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: The ground state (3p)\n",
    "\n",
    "In this final task, you will compare all the earlier techniques (Tasks 2-5) in predicting the ground state of candidate clusters.The ground-state structure refers to the structure with the lowest energy for a given concentration. Macroscopic properties like thermal conductivity, heat-capacity etc can depend strongly on the particular ground-state configuration and it is therefore important to know the correct ground-state structure.\n",
    "\n",
    "In the database `ground_states_candidates.db` you will find potential ground-state candidates. In this task you will assign each ground-state candidate in the database an estimated energy, and use your results from task 4 to estimate the probability of each candidate being the actual ground-state.\n",
    "\n",
    "**Tasks:**\n",
    "* Use the models that you've studied in Tasks 2-5 to predict the ground state structure and energy.\n",
    "    * From Task 2, use OLS and the RIDGE model with optimal hyperparameter that you found. \n",
    "    * From Task 3, use the optimal Covariance model that you found.\n",
    "    * From Task 4, use the cluster-expansions samples (from MCMC) and compute the frequency of each candidate structure being the ground-state.\n",
    "        * Also plot the disitrubtion of the ground-state energy.\n",
    "    * From Task 5, use the ARDR model with the optimal hyperparameter that you found.\n",
    "* Compare the results from predicting the ground state with Task 2-5 and discuss the results. \n",
    "\n",
    "**Discuss:**\n",
    "* Can you spot any differences between the models in predicting the ground state? Why/why not?\n",
    "* Which model approach do you think is the most suitable for this problem? (No right or wrong answer here; it's the discussion that's important)\n",
    "    * In particular elaborate on the differences between automatic feature selection (such as Ridge/ARDR) and more hands-on, physical intuition based approaches (such as Covariance/Full Bayesian).\n",
    "\n",
    "*Hints*\n",
    "* Remember that the ECIs have previously been normalized. You will thus have to normalize the ground state candidate cluster vectors and transform the predicted energies back to the original energy scale. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
